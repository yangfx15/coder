大家好，我是小❤。

## 引言

作为后台开发者，你是否曾在调试短视频功能时，被用户反馈的 “上传半天没反应”“视频播放卡顿” 搞得焦头烂额？

在抖音、快手这类日活亿级的短视频平台中，用户每一次点击 “发布” 按钮，背后都藏着一套复杂且精密的技术流程 —— `视频上传与转码`。

**这两个环节看似只是 “传文件” 和 “改格式”，实则是决定用户体验的关键。**

试想，若上传速度慢到几分钟都传不完一个 10MB 的视频，用户大概率会放弃创作；若转码后视频清晰度适配差，在弱网环境下满屏马赛克，观看体验也会大打折扣。

今天，小❤就从技术底层出发，拆解短视频系统中视频上传与转码的全流程，从时序图梳理到关键技术细节，再到性能优化实践，带你搞懂如何搭建一套稳定、高效的视频生产链路。



## 一、视频上传与转码整体时序图

要理解整个流程，首先得看清各组件之间的协作关系。以下时序图覆盖了从 “客户端发起上传” 到 “转码完成入库” 的全链路，涉及客户端、API 网关、上传服务等 7 类核心组件，每一步都标注了关键技术选型和数据流向，让你一目了然。

![image-20250827215838594](imgs/image-20250827215838594.png)



### 第一步：客户端发起上传请求，获取分片上传凭证（对应时序图 1-6 步）

![image-20250827222043970](imgs/image-20250827222043970.png)

#### 涉及模块及作用

- **Client（客户端）**：用户操作的入口（如抖音 APP），核心作用是收集视频元数据（大小、格式、整体 MD5），并向网关发起 “申请分片上传” 的请求 —— 这一步是为了避免直接上传大文件时的风险，先拿到 “上传许可”。

- **APIGW（API 网关）**：系统的 “入口守卫”，作用是统一接收客户端请求，先做两件关键事：①鉴权（验证用户 Token 是否有效，防止非法上传）；②限流（比如限制单个用户每秒最多发起 2 次上传请求，避免恶意刷量），之后再将请求转发给上传服务。

- **UploadSvc（上传服务）**：上传流程的 “协调者”，不直接存储视频，而是负责与对象存储交互，申请 “分片上传凭证（UploadId）”——UploadId 是对象存储分配的唯一标识，用于后续分片上传、合并的身份验证，避免不同用户的分片混淆。

- **OSS（对象存储）**：视频分片的 “临时仓库”，作用是生成并返回 UploadId 和分片编号范围（比如 1-10 片，对应 10MB 视频按 1MB 拆分），同时提供后续分片上传的接口地址。



#### 交互逻辑与设计原因

##### 为什么客户端不直接调用 OSS，而是要通过 API 网关和上传服务？

核心是 “安全与可控”：如果客户端直接对接 OSS，会暴露 OSS 的访问密钥，存在数据泄露风险；通过网关和上传服务中转，既能统一鉴权、限流，又能隐藏后端存储细节，后续切换存储服务商（如从阿里云 OSS 换成腾讯云 COS）时，客户端无需修改代码。



##### 为什么要先申请 UploadId？

分片上传是 “分多次传碎片”，UploadId 相当于 “碎片的归属标识”—— 只有携带正确 UploadId 的分片，OSS 才会认为是同一视频的碎片，后续才能合并成完整文件，避免不同视频的分片混在一起。



### 第二步：客户端分片上传视频数据（对应时序图 7-8 步，循环执行）

![image-20250827222112346](imgs/image-20250827222112346.png)

#### 涉及模块及作用

- **Client（客户端）**：按 1MB / 片拆分视频，并发发起分片上传请求（每次传 1 个分片，携带 UploadId、分片号、分片数据）—— 并发上传是为了提速，比如同时传 3 个分片，比一个一个传快 2-3 倍。

- **OSS（对象存储）**：接收并存储单个分片，校验分片的完整性（比如通过分片 MD5），之后返回 “ETag 值”——ETag 是分片的唯一标识，相当于 “碎片的签收单”，客户端后续需要用 ETag 证明 “这个分片已传成功”。



#### 为什么 OSS 要返回 ETag？

为了 “数据一致性校验”：客户端后续通知 “上传完成” 时，会把所有分片的 ETag 列表传给上传服务，上传服务再拿给 OSS——OSS 会对比自己存储的分片 ETag 和客户端上报的 ETag，只有全部一致，才会执行合并，避免分片丢失或损坏。



### 第三步：客户端完成上传，触发分片合并（对应时序图 9-15 步）

![image-20250827222129435](imgs/image-20250827222129435.png)

#### 为什么要通过 Kafka 发转码任务，而不是上传服务直接调用转码服务？

避免 “服务耦合” 和 “单点故障”：如果上传服务直接调用转码服务，转码服务宕机时，上传服务会被阻塞，甚至拖垮；用 Kafka 做中间件，上传服务发完消息就 “解脱”，转码服务恢复后可以继续消费任务，不会丢失；同时，后续增加多个转码节点时，只需让它们共同消费 Kafka 的任务，就能实现负载均衡。



#### 为什么上传服务要生成视频 ID？

为了 “全链路关联”：视频从上传、转码、审核到播放，涉及多个服务（上传、转码、审核、分发），视频 ID 是唯一的 “身份证”—— 转码服务通过视频 ID 关联转码任务和结果，审核服务通过视频 ID 关联待审核视频，客户端通过视频 ID 查询播放地址，确保全流程不会 “张冠李戴”。



### 第四步：转码服务消费任务，处理多码率转码（对应时序图 16-20 步）

![image-20250827222152547](imgs/image-20250827222152547.png)

#### 涉及模块及作用

- **TranscodeSvc（转码服务）**：从 Kafka 中消费转码任务（按优先级拉取，热门用户任务优先），先通过 OSS SDK 读取视频源文件，再调用 FFmpeg 工具进行多码率转码 —— 转码的核心是 “适配不同网络环境”，比如 480P 给弱网用户，1080P 给 5G 用户。



#### 为什么要按优先级拉取任务？

保障 “核心用户体验”：热门用户（如粉丝 10 万 + 的创作者）的视频上线速度很重要，若和普通用户的任务一起排队，可能会延迟很久，影响创作者积极性；按优先级排序后，热门用户的任务先处理，普通用户的任务后续处理，平衡 “核心需求” 和 “普通需求”。



#### 为什么要同时转 480P/720P/1080P 多码率？

适配 “多样化网络与设备”：用户的网络环境差异大（2G/3G/4G/5G/WiFi），设备屏幕分辨率也不同（手机 / 平板 / 电脑）—— 弱网用户用 480P，播放不卡顿；高速网络用户用 1080P，体验高清；多码率并存，能让不同场景的用户都有好体验。



#### FFmpeg 转码

FFmpeg 是行业标准的音视频处理工具，支持几乎所有格式的转码，且可以通过参数优化（如 H.265 编码），在保证画质的同时减少文件大小，节省存储和带宽成本。



### 第五步：转码完成，写入 CDN 源站，更新元数据（对应时序图 21-25 步）

![image-20250827222223103](imgs/image-20250827222223103.png)

#### 涉及模块及作用

- **CDNSource（CDN 源站）**：存储转码后的视频文件，后续会同步到全国的 CDN 节点（如北京、上海、广州的节点）—— 用户播放视频时，会从就近的 CDN 节点拉取，减少延迟（比如广州用户从广州节点拉取，比从北京源站拉取快 1-2 秒）。

- **Kafka（消息队列）**：接收上传服务发送的 “审核任务消息”—— 视频上线前必须审核（防止违规内容），通过 Kafka 转发审核任务，让审核服务异步处理，不阻塞上传服务的后续操作。



#### 为什么要把视频放到 CDN，而不是直接从 OSS 播放？

降低 “源站压力” 和 “用户延迟”：如果所有用户都从 OSS 拉取视频，OSS 的带宽压力会极大，且跨地域访问延迟高（如新疆用户访问北京 OSS，延迟可能 100ms+）；CDN 有全国节点，用户就近访问，延迟低（通常 20-50ms），且 CDN 能缓存热门视频，减少对 OSS 的请求，降低整体成本。



#### 为什么转码完成后要触发审核任务？

合规性要求：短视频平台必须审核内容（如是否含暴力、色情、违法信息），若直接上线未审核的视频，会有法律风险；转码完成后立即触发审核，审核通过后再对用户可见，既保障合规，又不影响后续流程（审核期间，视频处于 “待审核” 状态，用户看不到）。



### 时序图背后的核心设计思想

整个时序图的交互逻辑，围绕三个核心目标展开：

1. **可靠性**：通过分片上传、ETag 校验、任务重试，确保视频数据不丢失、不损坏；

1. **效率性**：通过并发上传、GPU 转码、CDN 分发，让视频上传快、转码快、播放快；

1. **可扩展性**：通过 API 网关、消息队列、微服务拆分（上传 / 转码 / 审核独立），后续增加节点、切换组件（如换 OSS、换 CDN）时，系统能灵活应对，支撑业务增长（从百万级用户到亿级用户）。

理解这些模块作用和交互逻辑，就能掌握短视频上传与转码的核心设计思路，后续遇到类似场景（如大文件上传、音视频处理），也能复用这些技术方案。



## 二、关键技术细节拆解

看懂了时序图，我们再深入每个环节的技术核心，解决实际开发中可能遇到的 “坑”。

### 2.1 分片上传的核心设计：解决 “大文件上传失败” 问题

短视频文件通常在 10-50MB 之间，若采用 “一次性上传”，一旦遇到网络波动，就可能导致全量重传，用户体验极差。分片上传通过三大设计，从根本上解决这一问题：

- **分片大小选择**：我们采用 1MB / 片的规格，这是兼顾并发效率与对象存储接口限制的最优解。以阿里云 OSS 为例，单分片最大支持 5GB，但小分片更易重试 —— 哪怕某一个 1MB 的分片上传失败，重新上传也只需几秒，而非整个几十 MB 的视频。

- **断点续传**：客户端会记录每一个已成功上传分片的 ETag（对象存储返回的唯一标识），下次上传同一视频时，只需对比本地分片 MD5 与 OSS 中的 ETag，仅重新上传未成功的分片。比如用户上传到一半退出 APP，再次打开时，系统能自动续传剩余分片，无需从头开始。

- **数据一致性校验**：两层校验保障文件完整。一是客户端上传前计算视频整体 MD5，与服务端预存的 MD5 比对，避免文件损坏后无效上传；二是分片合并时，OSS 会自动校验所有分片的 ETag 是否与客户端上报的一致，只要有一个分片不匹配，就拒绝合并，防止最终视频文件损坏。



### 2.2 转码服务的架构设计：兼顾效率与资源成本

转码是典型的 CPU 密集型操作，在高峰期，每秒可能有上万条转码任务涌入，若架构设计不合理，要么转码慢到用户等不及，要么资源消耗过高导致成本飙升。我们通过 “集群化 + 异步化” 的设计，平衡效率与成本：

- **任务调度策略**：采用 “优先级队列” 机制，根据用户等级和视频热度分配资源。热门用户（粉丝数 > 10 万）的视频转码任务设为 “高优先级”，确保其内容快速上线；普通用户任务设为 “中优先级”；归档视频（超过 3 个月无播放）设为 “低优先级”，错峰处理。同时，转码服务通过 Kafka 分区实现任务分片，每个转码节点只消费指定分区的任务，避免重复处理和资源争抢。

- **转码参数优化**：编码格式和码率是影响视频质量与存储成本的关键。我们选择 H.265（HEVC）编码，相比常用的 H.264，能节省 30% 的存储空间，且支持移动端硬解码，播放更流畅。码率则分为三档：480P（800kbps）适配 2G/3G 弱网环境，720P（1.5Mbps）应对主流 WiFi，1080P（3Mbps）满足 5G / 高速 WiFi 下的高清需求，客户端会根据用户当前网速自动切换。

- **资源隔离**：为避免不同业务争抢资源，我们将转码集群按业务线拆分，直播转码和短视频转码使用独立集群 —— 比如直播高峰期（晚上 8-10 点），不会占用短视频转码的 CPU 资源。同时，通过 Linux cgroups 限制单个转码任务的最大 CPU 使用率（不超过 20%），防止单个大视频转码拖垮整个节点。



### 2.3 异常处理机制：保障流程稳定性

在高并发场景下，异常是常态，一套完善的异常处理机制，能让系统在故障时 “不崩、不丢数据”：

- **上传失败重试**：分片上传超时（如 5 秒无响应）时，客户端会自动重试 3 次，且每次重试间隔指数递增（1 秒→2 秒→4 秒），减少网络波动的影响。若 3 次重试仍失败，上传服务会将该视频标记为 “上传异常”，并触发定时任务（每 10 分钟）重新发起分片合并，避免用户手动重试的麻烦。

- **转码失败降级**：若某一码率转码失败（比如 1080P 转码报错），转码服务不会中断整个任务，而是跳过该码率，优先保证 480P 和 720P 可用，同时记录失败日志并触发人工排查。对于转码超时的任务（如 30 分钟未完成，通常是超大文件），会自动终止并降级为 “低优先级”，待资源空闲时重新处理，不占用实时任务的资源。

- **数据一致性保障**：上传服务与转码服务通过 “视频 ID” 唯一关联，转码完成后，必须更新 MySQL 中该视频的 “转码状态”。若超过 5 分钟未更新，定时任务会重新发送转码任务，防止数据丢失。此外，若 OSS 分片合并失败，上传服务会自动删除已上传的分片，避免存储垃圾数据，同时通知客户端重新上传。



## 三、性能优化实践

光稳定还不够，还要 “快”—— 用户上传快、视频转码快、播放加载快，这需要针对性的性能优化。

### 3.1 客户端上传速度优化

- **并发分片上传**：客户端同时发起 3-5 个分片上传请求（根据网络环境动态调整，弱网时降为 1-2 个），相比串行上传，速度能提升 2-3 倍。比如一个 10MB 的视频，拆分为 10 个 1MB 分片，并发上传只需 2-3 秒，而串行上传可能需要 5-6 秒。

- **QUIC 协议替代 HTTP/2**：在弱网环境（如地铁、偏远地区），HTTP/2 的 TCP 连接容易丢包重传，我们改用 QUIC 协议（基于 UDP），支持 0-RTT 连接建立，上传成功率能提升 15% 以上（实测数据）。比如在地铁中，HTTP/2 上传成功率约 70%，QUIC 能提升到 85% 以上。

- **预上传校验**：客户端上传前先校验视频格式（仅支持 MP4/AVI/MOV），不符合格式的直接提示用户，避免无效上传。同时提前计算分片 MD5，若发现分片损坏，直接丢弃并重新分片，减少上传后校验失败的概率。



### 3.2 转码效率优化

- **GPU 加速转码**：对热门视频（日播放量 > 10 万），我们采用 NVIDIA Tesla T4 GPU 进行转码，相比传统 CPU（Intel Xeon 8375C），速度快 4-5 倍。比如一个 5 分钟的视频，CPU 转码需要 10 秒，GPU 转码只需 2 秒，大幅缩短视频上线时间。

- **转码任务批量处理**：当同一创作者发布多个视频时，我们会将这些视频的转码任务合并，复用一次 OSS 连接和 FFmpeg 初始化资源，减少 IO 开销。比如某创作者一次发布 3 个视频，批量处理比单独处理能节省 30% 的时间。

- **热点视频预转码**：通过用户行为预测提前触发转码。比如某创作者历史视频播放量均破万，当其刚发布新视频时，系统会预判该视频会成为热点，提前启动转码，避免用户等待 —— 原本需要等转码完成才能观看，现在发布后 1-2 秒就能加载播放。



## 小结

视频上传与转码作为短视频系统的 “内容入口”，是技术落地的关键一环。从分片上传解决大文件传输问题，到转码服务平衡效率与成本，再到异常处理和性能优化保障体验，每一步都需要结合业务场景做精细化设计。

对于后台开发者而言，理解这套流程不仅能应对 “搭建短视频系统” 的需求，更能将其中的技术思路（如分片传输、异步调度、资源隔离）迁移到其他大文件处理场景（如直播、云存储）。

当然，这只是短视频系统的冰山一角，后续我们还会拆解视频推荐、用户互动等核心模块。如果你在实际开发中遇到了具体问题，或者对某一技术点有更深入的探讨需求，欢迎在评论区留言，我们一起交流进步！