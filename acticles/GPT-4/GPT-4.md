### 1. GPT-4 面世

#### 1.1 GPT-4 模型发布，科技行业热火朝天

北京时间 3 月 15 日凌晨 1 点，OpenAI 总裁兼联合创始人 Greg Brockman 在推特上发帖开始推行 GPT-4：

![image-20230315100220154](img\twitter.png)

相比于 ChatGPT，GPT-4 在文字问答上可以接收 25 万字，有着近 8 倍的能力提升，这对语义理解模型、数据量和优质数据的筛选都有着很高的要求，因为文字问答和文字生成是完全不同的概念，但 GPT-4 做到了这一点，还做得不错。

我们知道 ChatGPT 是基于大规模语言模型的对话系统，其中训练时的海量数据、超大的规模架构和技术长期的积累，是目前国内大部分做 ChatGPT 的厂商遇到的难点。



#### 1.2 GPT-4 能力大增，可解析更多文本，也可识别图片

而 GPT-4，又是在 ChatGPT 的基础上进一步优化了架构模型，加大了训练数据的结果。不仅如此，GPT-4 还支持图像识别，并理解图片的内容。

GPT-4 介绍视频：

<video src="img\gpt-4.mp4">GPT-4[openAI]</video>

> OpenAI 的工程师也说道：GPT-4 是世界第一款高体验，强能力的先进 AI 系统，我们希望尽快把它推向所有人



视频中举例了 GPT-4 模型的最新能力，比如解决一些生活难题：

> 如何清洁装满食人鱼的水箱（How do you clean the inside of a tank filled with piranhas）？

GPT-4 给出了完整答案：

1. 将绳子系在金属垫圈上（Tie a string to a metal washer）
2. 将金属垫圈粘在海绵上（Glue the metal washer to a sponge）
3. 将海绵放入水族箱（Lower the sponge into the aquarium）
4. 使用磁铁引导海绵（Use a magnet to guide the sponge）
5. 用绳子取回海绵（Retrieve the sponge with the string）



除了解决日常难题，GPT-4 还提供了哥特式浪漫故事的写作能力，编写 python 脚本做数据分析的能力。

> 写一个关于电脑的哥特式浪漫故事（Write a gothic romance about a computer）
>
> 编写一个 python 脚本来分析我每月的购买量（Write a python script to analyze my monthly purchases）



并且在图片识别，和解决物理问题时也毫不含糊：

![image-20230315095514684](img\image-20230315095514684.png)

> Q：What happens when the glove drops，这个手套掉下来会发生什么？
>
> A：It will hit the wood plank and the ball wil fly up，击中木板并弹飞气球。



### 2. GPT发展历程

**五年技术积累，一朝闻名天下**

GPT, 全称是 Generative Pre-Trained Transformer，即根据基础算法模型 transformer，使用预训练技术得到的通用文本模型。

> 2018 年 6 月，OpenAI 发布了第一版 GPT-1

GPT-1 使用了 transformer 模型（NLP、翻译软件等也在用）的 decoder 架构+任务微调的形式，简单来说就是输入一种语言的句子，然后将其翻译成另外一种语言进行输出。

并且 GPT-1 在任务过程中，会把上游句子的结果作为参数（当然中间进行了一些转换，比如句子向量）作为新特征传输到下一次任务中，以此来训练模型的准确度，相当于机器学习。

**这时的 GPT 还没有什么特别出彩之处，使用的模型也是“平平无奇”，毕竟当时如火如荼的 NLP，也用到了 transformer。**



> 到了 2019 年 2 月，第二版 GPT-2 发布

相比第一版，OpenAI 优化了 decoder 网络架构，以及增加了数据规模约 10 倍，具体呈现在预训练数据和任务微调上。

整体来说，GPT-2 是一个不小的优化，但和谷歌同期产品 BERT 的对比，还是稍显逊色，所以 GPT-2 也没有登上 AI 模型的热榜第一名。



> 到了 20 年 5 月，OpenAI 发布了 GPT-3

一年多以后，OpenAI 的 GPT-3 带着更优的网络架构，更大的数据规模（比起 GPT-2 约增加了 100 倍）出场了，这次训练出的模型已经初露峥嵘，也奠定了 GPT 帝国的基础。

但实际上，GPT-3 和 GPT 第二版的模型和训练方式上没有本质上的区别，只是训练的数据更多了，模型内容更大了。



> 2022 年 11 月 30 日，ChatGPT 正式发布

GPT-3 两年以后以后，OpenAI 又发布了 ChatGPT 和 InstructGPT 这对姐妹模型，其中 ChatGPT 是 GPT-4 之前发布的一个预热模型，又叫做 GPT-3.5。

相比 GPT-3，ChatGPT 使用了指示学习和人工反馈的强化学习来指导模型的训练，共分为三个步骤：

* 有监督微调（SFT）
* 奖励机制（RM）训练
* 通过 PPO 根据奖励模型进行强化学习

总而言之，这时的 ChatGPT 已经能听懂人话、识别代码，做一些基本问答和小作文写作的工作了。

以下是 GPT-3.5 模型的对话问答：

![image-20230315114906603](img\image-20230315114906603.jpg)

做二元一次方程：

![image-20230315115019391](img\image-20230315115019391.png)

菜谱：

![image-20230315115123210](img\image-20230315115123210.png)

当然，也会出现一些意料之外的对话（写情诗，你给我看这个？）

![image-20230315115150955](img\image-20230315115150955.png)

对对联：

![image-20230315115805876](img\image-20230315115805876.png)

相比 GPT-3，ChatGPT 已经可以做一些简单问答，基本算术以及作文生成相关的工作了。但偶尔也会一本正经的胡说八道，并且对自己的回答坚信不疑。

![img](img\santi.png)

> 根据小说记载，三体人会在四百年后到达地球，当时的时间背景是2007年，所以ChatGPT的答案完全不对。



要实现以上对话，ChatGPT 至少集成了五个方面的模型能力：

| 模型                  | 能力             | 想要达到的效果                         |
| --------------------- | ---------------- | -------------------------------------- |
| GPT-3                 | 自然语言基础模型 | 理解人类语言，让 GPT 说话能够自然流畅  |
| Codex                 | 代码语言基础模型 | 理解机器代码，给代码找BUG/生成简单代码 |
| davinci-instruct-beta | 监督下的指令微调 | 听从人类指令，并生成答案               |
| text-davinci-001/002  | 人类反馈指令微调 | 生成人类更喜欢接受的答案               |
| text-davinci-003      | 强化学习指令微调 | 持续强化上述的对话能力                 |



> 2023 年 3 月 15 日，GPT-4 发布

GPT-4 在 GPT-3.5 的模型基础上，不仅模型更为安全可靠，文本更富有创意，而且还扩展了新能力。它的主要优化和突破有：

1. 更大模型加持：GPT-4 达到了 1700 亿+个模型参数，相比 GPT-3 大了约 10 倍，在处理大规模文本数据、文本生成和语言建模方面性能更好；
2. 训练效率更高效：GPT-4 用了更优的训练技术和算法模型，使得训练效率极大提升，在处理大规模文本数据和模型训练时更加可靠；
3. 数据安全做的更好：GPT-4 在数据加密和隐私文本上做了更多处理，使得其对话的安全性大大增加；
4. 新增多模态模型能力：除了文本，GPT-4 还可接收图像输入，并对其进行图像内容识别，然后以文本的方式输出答案。

为了比对两个模型之间的应用区别，OpenAI 还根据不同场景的测试和专业考试上对它们做了分析。

比如，一些专业测评和学术基准测试上，GPT-4 的表现和人类水平相当：其中包括但不限于**律师专业测试，SAT 测试（美国高考），GPT-4 的分数都在应试者前 10% 左右**。其中，SAT 阅读考试甚至拿到了 710+ 的水平（分数线前 7%），上斯坦福都轻轻松松。相比而言， GPT-3.5 的得分只能在倒数 10% 左右。

![img](img\exam.png)

和人类对比，GPT 在这么短的时间内产生飞跃式的成绩跳跃，会被怀疑在考试时附近坐了个学霸。

而 GPT-4 不仅是学霸，还是学神，它可以在短时间内读完一整篇 InstructGPT 的论文，并总结出摘要。它还可以快速阅读税务法则，来计算夫妻一年要缴纳多少税，并且写出计算过程。对此，Greg Brockman 大为赞赏，因为这些税务文件，他自己读了半个小时也没搞懂，然而 GPT-4 可以快速给出答案。

在 GPT-4 发布的公告中，OpenAI 说明了这个模型训练完成于 2022 年 8 月，已经接受了 6 个月左右的安全培训，在内部对抗真实性的评估中，GPT-4 的得分比 ChatGPT 高出很多。这也意味着，GPT-4 的商业化进程，又在前面的基础上前进了一大步。

接下来，可能就是成本和需求实现完善度的衡量了，毕竟，ChatGPT 一条问答的初始成本就需要几毛钱，训练一次的代价就是几百万美金，而 GPT-4 的模型版本更大，数据更多，运行起来的成本也会越发高昂。



> 微软 Bing 搜索已经集成 GPT-4

微软对 OpenAI 的投资总计高达一百多亿美金，约合 900 亿元。在 GPT-4 发布的几周前，微软已经在新版必应搜索用上了 GPT-4，微软 CEO 纳德拉表示：它们的搜索已经优于谷歌了！

![img](img\bing.png)

所以，如果你并非开发者或者付费用户，但实在想尝鲜，微软的新版 Bing 或许是一个很好的选择。



### 3. 智能化和商用化

回首 GPT 的发展历程，我们发现 GPT 一直是在围绕**搜索和创造**两大方面来不断完善模型，OpenAI 的工程师也说过：GPT 旨在放大人类的可能，实现科技的价值。

想象一下，如果你的孩子读 5 年级，拥有一个无限耐心的 GPT 教学助手一定是非常美好的事情。

这只是在教培领域，随着 GPT 的不断优化，它未来在商业化领域也必定会遍地开花。比如护工行业，GPT-4 的视觉功能已经可以帮助盲人和低视力用户；互联网行业，GPT 可进行商业化的数据分析、代码编程等；自媒体行业，GPT 可以自动生成用户喜欢的宣传和推广文案。

未来百般可能，我们拭目以待！



